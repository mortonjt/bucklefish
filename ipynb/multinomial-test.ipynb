{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mortonjt/miniconda3/envs/bayesian-regression/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/mortonjt/miniconda3/envs/bayesian-regression/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from bayesian_regression.util.generators import random_poisson_model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from biom import load_table, Table\n",
    "from sklearn.utils import check_random_state\n",
    "from scipy.sparse import coo_matrix\n",
    "from skbio.stats.composition import _gram_schmidt_basis, closure\n",
    "import edward as ed\n",
    "import tensorflow as tf\n",
    "from tqdm import trange, tqdm\n",
    "import sys\n",
    "from gneiss.cluster import rank_linkage\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation\n",
    "\n",
    "We will first propose a generative model model species distributions.\n",
    "\n",
    "The model will be constructed as follows\n",
    "\n",
    "Generate a linear regression given known regression coefficients and intercepts (B) and known environmental gradients (G)\n",
    "\n",
    "$v = GB$\n",
    "\n",
    "$v$ represents the log ratios (also known as logits) of the species.  These logits can ultimately be used to interpret trends of ecological succession amongst groups of species.  However, these log ratios cannot be directly estimated (since it isn't possible to take the logarithm of zero).  Instead, we will rely on the generative process, and convert these log ratios to proportions, before generating counts.\n",
    "\n",
    "$v$ can be converted to proportions using the ILR transform.  $v$ has $D-1$ dimensions, where $D$ is the number of species.  The ILR transform can be represented as follows\n",
    "\n",
    "$\\eta = \\Psi v$\n",
    "\n",
    "where $\\Psi$ is an orthonormal basis of dimensions $D \\times D-1$.  It is possible to encode a tree topology into $\\Psi$.  One approach is to take phylogenetic trees in order to infer evolutionary trends with respect to environmental gradients.  For the sake of simplicity, we will choose an arbituary tree.  In terms of generating microbial counts, the choice tree doesn't matter since all trees will generate orthonormal bases that are equivalent up to rotation.\n",
    "\n",
    "Once we have proportions estimated by $\\eta$, we can then use these proportions to parameterize a counting distribution, such as the Multinomial distribution, the Poisson distribution or the Negative Binomial distribution.\n",
    "\n",
    "Here we will use the Poisson distribution to simulate microbial counts.  In order to account or differences due to sequencing depth and additional sample biases, a bias constant $\\theta$ is added to the model.  A global bias $\\alpha$ is added to everything.\n",
    "\n",
    "$Y \\sim Poisson( exp(\\eta + \\theta + \\alpha) )$\n",
    "\n",
    "To evaluate how well the model fits, we will first simulate the data given the model.  We will randomly simulate the coefficients from a given prior distribution, and then generate the counts from the model.\n",
    "If the model works, we should be able to recover the underlying parameters, and regenerate the microbial count table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_multinomial_model(num_samples, num_features,\n",
    "                             tree=None,\n",
    "                             reps=1,\n",
    "                             low=2, high=10,\n",
    "                             alpha_mean=0,\n",
    "                             alpha_scale=5,\n",
    "                             theta_mean=0,\n",
    "                             theta_scale=5,\n",
    "                             gamma_mean=0,\n",
    "                             gamma_scale=5,\n",
    "                             kappa_mean=0,\n",
    "                             kappa_scale=5,\n",
    "                             beta_mean=0,\n",
    "                             beta_scale=5,\n",
    "                             seed=0):\n",
    "    \"\"\" Generates a table using a random poisson regression model.\n",
    "\n",
    "    Here we will be simulating microbial counts given the model, and the\n",
    "    corresponding model priors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_samples : int\n",
    "        Number of samples\n",
    "    num_features : int\n",
    "        Number of features\n",
    "    tree : np.array\n",
    "        Tree specifying orthonormal contrast matrix.\n",
    "    low : float\n",
    "        Smallest gradient value.\n",
    "    high : float\n",
    "        Largest gradient value.\n",
    "    alpha_mean : float\n",
    "        Mean of alpha prior  (for global bias)\n",
    "    alpha_scale: float\n",
    "        Scale of alpha prior  (for global bias)\n",
    "    theta_mean : float\n",
    "        Mean of theta prior (for sample bias)\n",
    "    theta_scale : float\n",
    "        Scale of theta prior (for sample bias)\n",
    "    gamma_mean : float\n",
    "        Mean of gamma prior (for feature bias)\n",
    "    gamma_scale : float\n",
    "        Scale of gamma prior (for feature bias)\n",
    "    kappa_mean : float\n",
    "        Mean of kappa prior (for overdispersion)\n",
    "    kappa_scale : float\n",
    "        Scale of kappa prior (for overdispersion)\n",
    "    beta_mean : float\n",
    "        Mean of beta prior (for regression coefficients)\n",
    "    beta_scale : float\n",
    "        Scale of beta prior (for regression coefficients)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    table : biom.Table\n",
    "        Biom representation of the count table.\n",
    "    metadata : pd.DataFrame\n",
    "        DataFrame containing relevant metadata.\n",
    "    beta : np.array\n",
    "        Regression parameter estimates.\n",
    "    theta : np.array\n",
    "        Bias per sample.\n",
    "    gamma : np.array\n",
    "        Bias per feature\n",
    "    kappa : np.array\n",
    "        Dispersion rates of counts per sample.\n",
    "    \"\"\"\n",
    "    # generate all of the coefficient using the random poisson model\n",
    "    state = check_random_state(seed)\n",
    "    alpha = state.normal(alpha_mean, alpha_scale)\n",
    "    theta = state.normal(theta_mean, theta_scale, size=(num_samples, 1))\n",
    "    beta = state.normal(beta_mean, beta_scale, size=num_features-1)\n",
    "    gamma = state.normal(gamma_mean, gamma_scale, size=num_features-1)\n",
    "    kappa = state.lognormal(kappa_mean, kappa_scale, size=num_features-1)\n",
    "\n",
    "    if tree is None:\n",
    "        basis = coo_matrix(_gram_schmidt_basis(num_features), dtype=np.float32)\n",
    "    else:\n",
    "        basis = sparse_balance_basis(tree)[0]\n",
    "\n",
    "    G = np.hstack([np.linspace(low, high, num_samples // reps)] \n",
    "                  for _ in range(reps))\n",
    "    G = np.sort(G)\n",
    "    N, D = num_samples, num_features\n",
    "    G_data = np.vstack((np.ones(N), G)).T\n",
    "    B = np.vstack((gamma, beta))\n",
    "\n",
    "    mu = G_data @ B @ basis\n",
    "    # we use kappa here to handle overdispersion.\n",
    "    #eps = lambda x: state.normal([0] * len(x), x)    \n",
    "    eps_ = np.vstack([state.normal([0] * len(kappa), kappa)\n",
    "                      for _ in range(mu.shape[0])])\n",
    "    eps = eps_ @ basis\n",
    "    depth = np.exp(alpha).astype(np.int32)\n",
    "    table = np.vstack(\n",
    "        state.multinomial(depth,\n",
    "            closure(np.exp(\n",
    "                mu[i, :] + eps[i, :]               \n",
    "            ))\n",
    "        )\n",
    "        for i in range(mu.shape[0])        \n",
    "    ).T\n",
    "\n",
    "    samp_ids = ['S%d' % i for i in range(num_samples)]\n",
    "    feat_ids = ['F%d' % i for i in range(num_features)]\n",
    "    balance_ids = ['L%d' % i for i in range(num_features-1)]\n",
    "\n",
    "    table = Table(table, feat_ids, samp_ids)\n",
    "    metadata = pd.DataFrame({'G': G.ravel()}, index=samp_ids)\n",
    "    beta = pd.DataFrame({'beta': beta.ravel()}, index=balance_ids)\n",
    "    gamma = pd.DataFrame({'gamma': gamma.ravel()}, index=balance_ids)\n",
    "    kappa = pd.DataFrame({'kappa': kappa.ravel()}, index=balance_ids)\n",
    "    theta = pd.DataFrame({'theta': theta.ravel()}, index=samp_ids)\n",
    "    return table, metadata, basis, alpha, beta, theta, gamma, kappa, eps_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sparse_balance_basis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1864bc08507c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                                \u001b[0mkappa_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                \u001b[0mbeta_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                                \u001b[0mbeta_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                               ) \n\u001b[1;32m     21\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_kappa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_eps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b99168c769f1>\u001b[0m in \u001b[0;36mrandom_multinomial_model\u001b[0;34m(num_samples, num_features, tree, reps, low, high, alpha_mean, alpha_scale, theta_mean, theta_scale, gamma_mean, gamma_scale, kappa_mean, kappa_scale, beta_mean, beta_scale, seed)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mbasis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_gram_schmidt_basis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mbasis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_balance_basis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     G = np.hstack([np.linspace(low, high, num_samples // reps)] \n",
      "\u001b[0;31mNameError\u001b[0m: name 'sparse_balance_basis' is not defined"
     ]
    }
   ],
   "source": [
    "num_samples = 100\n",
    "num_features = 1000\n",
    "feat_ids = ['F%d' % i for i in range(num_features)]\n",
    "ranks = pd.Series(np.arange(num_features), index=feat_ids)\n",
    "tree = rank_linkage(ranks, method='average')\n",
    "res = random_multinomial_model(num_samples, num_features,\n",
    "                               tree=tree,\n",
    "                               reps=1,\n",
    "                               low=-1, high=1,\n",
    "                               alpha_mean=4,\n",
    "                               alpha_scale=1,\n",
    "                               theta_mean=0,\n",
    "                               theta_scale=1,\n",
    "                               gamma_mean=0,\n",
    "                               gamma_scale=1,\n",
    "                               kappa_mean=0,\n",
    "                               kappa_scale=0.5,\n",
    "                               beta_mean=0,\n",
    "                               beta_scale=1 \n",
    "                              ) \n",
    "table, metadata, basis, sim_alpha, sim_beta, sim_theta, sim_gamma, sim_kappa, sim_eps = res\n",
    "\n",
    "N, D = num_samples, num_features\n",
    "p = metadata.shape[1]   # number of covariates\n",
    "G_data = metadata.values\n",
    "y_data = np.array(table.matrix_data.todense()).T\n",
    "\n",
    "save_path = 'tf_multinomial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the heatmap of the counts\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "plt.imshow(table.matrix_data.todense(), \n",
    "           aspect='auto', cmap='Reds', \n",
    "           norm=colors.LogNorm()\n",
    "          )\n",
    "plt.colorbar(label='counts')\n",
    "plt.ylabel('species', fontsize=24)\n",
    "plt.xlabel('samples', fontsize=24)\n",
    "plt.title('Simulation', fontsize=24) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "This is where we will specify the model.  Specifically\n",
    "\n",
    "$e_{ij} \\sim N(0, \\kappa_j)$\n",
    "\n",
    "$v_{ij} = \\gamma_j + g_{i.} \\cdot \\beta_{.j} + e_{ij}$\n",
    "\n",
    "$\\eta_i = \\Psi v_i + \\theta_i$\n",
    "\n",
    "$Y_i \\sim Poisson( \\exp(\\eta_i) )$\n",
    "\n",
    "$\\gamma$ represents the intercept terms and $\\beta$ represent the regression coefficient terms.\n",
    "\n",
    "G and Y are both observed.  Since we are estimating both $B$ and $\\theta$, we will need set priors for both of these variables.  We will set unit normal priors on both of these variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_matmul(A, B, row_index, col_index):\n",
    "    \"\"\" Sparse matrix multiplication.\n",
    "\n",
    "    This will try to evaluate the following product\n",
    "\n",
    "    A[i] @ B[j]\n",
    "\n",
    "    where i, j are the row and column indices specified in `indices`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : tf.Tensor\n",
    "       Left 2D tensor\n",
    "    B : tf.Tensor\n",
    "       Right 2D tensor\n",
    "    row_idx : tf.Tensor\n",
    "       Row indexes to access in A\n",
    "    col_idx : tf.Tensor\n",
    "       Column indexes to access in B\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "       Result stored in a sparse tensor format, where the values\n",
    "       are derived from A[i] @ B[j] where i, j are the row and column\n",
    "       indices specified in `indices`.\n",
    "    \"\"\"\n",
    "    A_flat = tf.gather(A, row_index, axis=0)\n",
    "    B_flat = tf.transpose(tf.gather(B, col_index, axis=1))\n",
    "    values = tf.reduce_sum(tf.multiply(A_flat, B_flat), axis=1)\n",
    "    return values\n",
    "\n",
    "def sparse_balance_basis(tree):\n",
    "    \"\"\" Calculates sparse representation of an ilr basis from a tree.\n",
    "\n",
    "    This computes an orthonormal basis specified from a bifurcating tree.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree : skbio.TreeNode\n",
    "        Input bifurcating tree.  Must be strictly bifurcating\n",
    "        (i.e. every internal node needs to have exactly 2 children).\n",
    "        This is used to specify the ilr basis.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scipy.sparse.coo_matrix\n",
    "       The ilr basis required to perform the ilr_inv transform.\n",
    "       This is also known as the sequential binary partition.\n",
    "       Note that this matrix is represented in clr coordinates.\n",
    "    nodes : list, str\n",
    "        List of tree nodes indicating the ordering in the basis.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        The tree doesn't contain two branches.\n",
    "\n",
    "    \"\"\"\n",
    "    # this is inspired by @wasade in\n",
    "    # https://github.com/biocore/gneiss/pull/8\n",
    "    t = tree.copy()\n",
    "    D = len(list(tree.tips()))\n",
    "    # calculate number of tips under each node\n",
    "    for n in t.postorder(include_self=True):\n",
    "        if n.is_tip():\n",
    "            n._tip_count = 1\n",
    "        else:\n",
    "            try:\n",
    "                left, right = n.children[NUMERATOR], n.children[DENOMINATOR],\n",
    "            except:\n",
    "                raise ValueError(\"Not a strictly bifurcating tree.\")\n",
    "            n._tip_count = left._tip_count + right._tip_count\n",
    "\n",
    "    # calculate k, r, s, t coordinate for each node\n",
    "    left, right = t.children[NUMERATOR], t.children[DENOMINATOR],\n",
    "    t._k, t._r, t._s, t._t = 0, left._tip_count, right._tip_count, 0\n",
    "    for n in t.preorder(include_self=False):\n",
    "        if n.is_tip():\n",
    "            n._k, n._r, n._s, n._t = 0, 0, 0, 0\n",
    "\n",
    "        elif n == n.parent.children[NUMERATOR]:\n",
    "            n._k = n.parent._k\n",
    "            n._r = n.children[NUMERATOR]._tip_count\n",
    "            n._s = n.children[DENOMINATOR]._tip_count\n",
    "            n._t = n.parent._s + n.parent._t\n",
    "        elif n == n.parent.children[DENOMINATOR]:\n",
    "            n._k = n.parent._r + n.parent._k\n",
    "            n._r = n.children[NUMERATOR]._tip_count\n",
    "            n._s = n.children[DENOMINATOR]._tip_count\n",
    "            n._t = n.parent._t\n",
    "        else:\n",
    "            raise ValueError(\"Tree topology is not correct.\")\n",
    "\n",
    "    # navigate through tree to build the basis in a sparse matrix form\n",
    "    value = []\n",
    "    row, col = [], []\n",
    "    nodes = []\n",
    "    i = 0\n",
    "\n",
    "    for n in t.levelorder(include_self=True):\n",
    "\n",
    "        if n.is_tip():\n",
    "            continue\n",
    "\n",
    "        for j in range(n._k, n._k + n._r):\n",
    "            row.append(i)\n",
    "            col.append(D-1-j)\n",
    "            A = np.sqrt(n._s / (n._r * (n._s + n._r)))\n",
    "\n",
    "            value.append(A)\n",
    "\n",
    "        for j in range(n._k + n._r, n._k + n._r + n._s):\n",
    "            row.append(i)\n",
    "            col.append(D-1-j)\n",
    "            B = -np.sqrt(n._r / (n._s * (n._s + n._r)))\n",
    "\n",
    "            value.append(B)\n",
    "        i += 1\n",
    "        nodes.append(n.name)\n",
    "\n",
    "    basis = coo_matrix((value, (row, col)), shape=(D-1, D), dtype=np.float32)\n",
    "\n",
    "    return basis, nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priors\n",
    "theta_mean = 0\n",
    "theta_scale = 2\n",
    "gamma_mean = 0\n",
    "gamma_scale = 2\n",
    "kappa_mean = 0\n",
    "kappa_scale = 10\n",
    "beta_mean = 0\n",
    "beta_scale = 2\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Mini batch size\n",
    "batch_size = 5 \n",
    "epoch = table.shape[1] / batch_size\n",
    "num_iter = int(epoch * batch_size)\n",
    "\n",
    "basis = basis.T\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=2, \n",
    "                        inter_op_parallelism_threads=2,\n",
    "                        device_count={'CPU': 4}) \n",
    "session = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from edward.models import Normal, Multinomial\n",
    "\n",
    "# Place holder variables to accept input data                                                                 \n",
    "G_ph = tf.placeholder(tf.float32, [batch_size, p], name='G_ph')\n",
    "Y_ph = tf.placeholder(tf.float32, [batch_size, D], name='Y_ph')\n",
    "total_count = tf.placeholder(tf.float32, [batch_size], name='total_count')\n",
    "\n",
    "# Define PointMass Variables first\n",
    "qgamma = tf.Variable(tf.random_normal([1, D]), name='qgamma')\n",
    "qbeta = tf.Variable(tf.random_normal([p, D]), name='qB')\n",
    "\n",
    "# Distributions\n",
    "# species bias\n",
    "gamma = Normal(loc=tf.zeros([1, D]) + gamma_mean,\n",
    "               scale=tf.ones([1, D]) * gamma_scale, \n",
    "               name='gamma')\n",
    "# regression coefficents distribution\n",
    "beta = Normal(loc=tf.zeros([p, D]) + beta_mean,\n",
    "              scale=tf.ones([p, D]) * beta_scale, \n",
    "              name='B')\n",
    "\n",
    "Bprime = tf.concat([qgamma, qbeta], axis=0)\n",
    "\n",
    "# add bias terms for samples\n",
    "Gprime = tf.concat([tf.ones([batch_size, 1]), G_ph], axis=1)\n",
    "\n",
    "eta = tf.matmul(Gprime, Bprime)\n",
    "phi = tf.nn.log_softmax(eta)\n",
    "Y = Multinomial(total_count=total_count, logits=phi, name='Y') \n",
    "\n",
    "loss = -(tf.reduce_sum(gamma.log_prob(qgamma)) + \\\n",
    "         tf.reduce_sum(beta.log_prob(qbeta)) + \\\n",
    "         tf.reduce_sum(Y.log_prob(Y_ph)))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "#global_step = tf.Variable(0, name=\"global_step\")    \n",
    "\n",
    "gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 10.0)\n",
    "train = optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "tf.summary.scalar('loss', loss)\n",
    "tf.summary.histogram('qbeta', qbeta)\n",
    "tf.summary.histogram('qgamma', qgamma)\n",
    "#tf.summary.histogram('gradients', gradients)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "writer = tf.summary.FileWriter(save_path, session.graph)\n",
    "\n",
    "losses = np.array([0.] * num_iter)\n",
    "idx = np.arange(metadata.shape[0])\n",
    "for i in tqdm(range(num_iter)):\n",
    "    batch_idx = np.random.choice(idx, size=batch_size)\n",
    "    _, summary, train_loss, grads = session.run(\n",
    "        [train, merged, loss, gradients], \n",
    "        feed_dict={\n",
    "            Y_ph: y_data[batch_idx].astype(np.float32), \n",
    "            G_ph: metadata.values[batch_idx].astype(np.float32),\n",
    "            total_count: y_data[batch_idx].sum(axis=1).astype(np.float32)\n",
    "        }\n",
    "    )\n",
    "    writer.add_summary(summary, i)\n",
    "    losses[i] = train_loss\n",
    "    #print(\"Step %d / %d: lr = %5.2f loss = %6.2f\\r\" %\n",
    "    #      (i, num_iter, lr, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict={\n",
    "    Y_ph: y_data[batch_idx].astype(np.float32), \n",
    "    G_ph: metadata.values[batch_idx].astype(np.float32),\n",
    "    total_count: y_data[batch_idx].sum(axis=1).astype(np.float32)\n",
    "}\n",
    "beta_ = qbeta.eval() @ basis\n",
    "gamma_ = qgamma.eval() @ basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "x = np.ravel(sim_beta)\n",
    "y = np.ravel(beta_)\n",
    "ax.scatter(x, y)\n",
    "mx = np.linspace(min([sim_beta.values.min(), beta_.min()]), \n",
    "                 max([sim_beta.values.max(), beta_.max()]))\n",
    "ax.plot(mx, mx, '-k')\n",
    "plt.xlabel('Simulated Beta')\n",
    "plt.ylabel('Estimated Beta')\n",
    "print(pearsonr(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "x = np.ravel(sim_gamma)\n",
    "y = np.ravel(gamma_)\n",
    "ax.scatter(x, y)\n",
    "mx = np.linspace(min([sim_gamma.values.min(), gamma_.min()]), \n",
    "                 max([sim_gamma.values.max(), gamma_.max()]))\n",
    "ax.plot(mx, mx, '-k')\n",
    "plt.xlabel('Simulated Gamma')\n",
    "plt.ylabel('Estimated Gamma')\n",
    "print(pearsonr(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
